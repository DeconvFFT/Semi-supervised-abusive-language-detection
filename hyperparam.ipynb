{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python397jvsc74a57bd0f1cd6fa187f75416e22a91ec71c95b3d72c0eb3110f2cc060653dcaa47999e2a",
   "display_name": "Python 3.9.7 64-bit ('tfnlp': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "f1cd6fa187f75416e22a91ec71c95b3d72c0eb3110f2cc060653dcaa47999e2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/saumyamehta/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/saumyamehta/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "olid_data = pd.read_csv(\"data/OLIDv1/olid-training-v1.0.tsv\", sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent, lemmatizer, stop_words):\n",
    "    \n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz 0123456789',.\"\n",
    "    sent = sent.lower()\n",
    "    sent = sent.replace('@user','')\n",
    "    sent = sent.replace('@[\\w\\-]+','')\n",
    "\n",
    "    cleaned_sent_list = [char if char in alphabet else ' ' for char in sent] # remove all tags not in the alphabet\n",
    "\n",
    "    cleaned_sent = ''.join(cleaned_sent_list)\n",
    "    cleaned_sent_list = [lemmatizer.lemmatize(token) for token in cleaned_sent.split(\" \")]\n",
    "    cleaned_sent_list = [word for word in cleaned_sent_list if not word in stop_words]\n",
    "    cleaned_sent = ' '.join(cleaned_sent_list)\n",
    "    cleaned_sent = cleaned_sent.replace(\"n't\",' not') # replace words like \"isn't\" with \"is not\"\n",
    "    cleaned_sent = ' . '.join([x for x in cleaned_sent.split('.') if len(x)>0]) # remove multiple periods, and add spaces before and after a period\n",
    "    cleaned_sent = ' , '.join([x for x in cleaned_sent.split(',') if len(x)>0]) # add spaces before and after a comma\n",
    "    cleaned_sent = ' '.join(cleaned_sent.split()) # remove multiple spaces\n",
    "    return cleaned_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emoji preprocess\n",
    "def emoji_to_text(s):\n",
    "    s = emoji.demojize(s)\n",
    "    s = s.replace(':',' ')\n",
    "    s = s.replace('_',' ')    \n",
    "    s = ' '.join(s.split())\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading twitter slang data\n",
    "slang_df = pd.read_csv('data/twitterSlang.csv')\n",
    "slang_dict = dict(zip(slang_df.slang, slang_df.formal_translation))\n",
    "\n",
    "def fix_slang(s):\n",
    "    s_list = s.split()\n",
    "    new_s_list = []\n",
    "    for word in s_list:\n",
    "        if word in slang_dict.keys():\n",
    "            new_s_list.append(slang_dict[word])\n",
    "        else:\n",
    "            new_s_list.append(word)\n",
    "            \n",
    "    return ' '.join(new_s_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent, lemmatizer, stop_words):\n",
    "    \n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz 0123456789',.\"\n",
    "    sent = emoji_to_text(sent)\n",
    "    sent = fix_slang(sent)\n",
    "    sent = sent.lower() \n",
    "    sent = sent.replace('@user','')\n",
    "    sent = sent.replace('@[\\w\\-]+','')\n",
    "\n",
    "    cleaned_sent_list = [char if char in alphabet else ' ' for char in sent] # remove all tags not in the alphabet\n",
    "\n",
    "    cleaned_sent = ''.join(cleaned_sent_list)\n",
    "    cleaned_sent_list = [lemmatizer.lemmatize(token) for token in cleaned_sent.split(\" \")]\n",
    "    cleaned_sent_list = [word for word in cleaned_sent_list if not word in stop_words]\n",
    "    cleaned_sent = ' '.join(cleaned_sent_list)\n",
    "    cleaned_sent = cleaned_sent.replace(\"n't\",' not') # replace words like \"isn't\" with \"is not\"\n",
    "    cleaned_sent = ' . '.join([x for x in cleaned_sent.split('.') if len(x)>0]) # remove multiple periods, and add spaces before and after a period\n",
    "    cleaned_sent = ' , '.join([x for x in cleaned_sent.split(',') if len(x)>0]) # add spaces before and after a comma\n",
    "    cleaned_sent = ' '.join(cleaned_sent.split()) # remove multiple spaces\n",
    "    return cleaned_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X train shape: (13240,), y train shape: (13240,)\n"
     ]
    }
   ],
   "source": [
    "#### Task A\n",
    "\n",
    "X_train = olid_data.tweet\n",
    "y_train = pd.factorize(olid_data.subtask_a)[0]\n",
    "\n",
    "X_test=pd.read_csv('data/OLIDv1/testset-levela.tsv',sep=\"\\t\").tweet\n",
    "y_test=pd.read_csv( 'data/OLIDv1/labels-levela.csv',header=None).iloc[:,-1]\n",
    "y_test = pd.factorize(y_test)[0]\n",
    "import collections\n",
    "collections.Counter(y_train)\n",
    "print(f'X train shape: {X_train.shape}, y train shape: {y_train.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X train shape: (4400,), y train shape: (4400,)\n"
     ]
    }
   ],
   "source": [
    "##### Task b\n",
    "# X_train = olid_data.tweet\n",
    "# y_train = olid_data.subtask_b\n",
    "\n",
    "# X_train = X_train[y_train.notna()]\n",
    "# y_train = y_train[y_train.notna()]\n",
    "# X_test=pd.read_csv('data/OLIDv1/testset-levelb.tsv',sep=\"\\t\").tweet\n",
    "# y_test=pd.read_csv( 'data/OLIDv1/labels-levelb.csv',header=None).iloc[:,-1]\n",
    "# y_test = pd.factorize(y_test)[0]\n",
    "# y_train = pd.factorize(y_train)[0]\n",
    "\n",
    "# import collections\n",
    "# collections.Counter(y_train)\n",
    "# print(f'X train shape: {X_train.shape}, y train shape: {y_train.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: 4.198473282442748, 1: 0.5675954592363261}\n"
     ]
    }
   ],
   "source": [
    "# from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "# from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# class_weights = class_weight.compute_class_weight(\n",
    "#     class_weight = 'balanced',\n",
    "#     classes = np.unique(y_train),\n",
    "#     y = y_train)\n",
    "\n",
    "# weights={}\n",
    "# for index, weight in enumerate(class_weights) :\n",
    "#   weights[index]=weight\n",
    "# print(weights)\n",
    "# #smt = SMOTE(random_state=777, k_neighbors=1)\n",
    "# #rus = RandomUnderSampler(random_state=777)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X train shape: (9268,), y train shape: (9268,)\nX valid shape: (3972,), y valid shape: (3972,)\nX valid shape: (860,), y test shape: (860,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train,y_train,test_size=0.3, random_state=42)\n",
    "# check shapes of train, test and validation data\n",
    "print(f'X train shape: {X_train.shape}, y train shape: {y_train.shape}')\n",
    "print(f'X valid shape: {X_valid.shape}, y valid shape: {y_valid.shape}')\n",
    "print(f'X valid shape: {X_test.shape}, y test shape: {y_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\")) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data_train = [preprocess(tweet,lemmatizer,stop_words) for tweet in X_train]\n",
    "data_valid = [preprocess(tweet,lemmatizer,stop_words) for tweet in X_valid]\n",
    "data_test = [preprocess(tweet,lemmatizer,stop_words) for tweet in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_size = 100\n",
    "lstm_output_dim = 32\n",
    "max_length = 280\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(data_train)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "valid_sequences = tokenizer.texts_to_sequences(data_valid)\n",
    "valid_padded = pad_sequences(valid_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(data_test)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: 1.510922725790675, 1: 0.7472988227705208}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# compute class weights : \"https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html\"\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight = 'balanced',\n",
    "    classes = np.unique(y_train),\n",
    "    y = y_train)\n",
    "class_weights = dict(zip(np.unique(y_train), class_weights))\n",
    "class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.bincount(y_train))\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# X_valid = scaler.transform(X_valid)\n",
    "\n",
    "# X_train = np.clip(X_train, -5, 5)\n",
    "# X_valid = np.clip(X_valid, -5, 5)\n",
    "# print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([100,150,200]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.05,0.2,0.5]))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],\n",
    "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(vocab_size, embedding_size, max_length, hparams):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size, embedding_size, input_length=max_length))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences = True)))\n",
    "    model.add(tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dropout(hparams[HP_DROPOUT]))\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(\n",
    "    optimizer=hparams[HP_OPTIMIZER],\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    model.fit(train_padded, y_train, batch_size=256,epochs=25,class_weight=class_weights,validation_data=(valid_padded, y_valid),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.TensorBoard('logs/hparam_tuning'),  # log metrics\n",
    "        hp.KerasCallback('logs/hparam_tuning', hparams),  # log hparams\n",
    "    ]) # Run with 1 epoch to speed things up for demo purposes\n",
    "    _, accuracy = model.evaluate(test_padded, y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    accuracy = train_test_model(10000, 100, 280,hparams)\n",
    "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- Starting trial: run-0\n",
      "{'num_units': 100, 'dropout': 0.05, 'optimizer': 'adam'}\n",
      "Epoch 1/25\n",
      "2021-12-08 11:25:18.188473: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-12-08 11:25:18.188485: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-12-08 11:25:18.188498: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      " 1/37 [..............................] - ETA: 52s - loss: 0.6978 - accuracy: 0.35572021-12-08 11:25:19.697605: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-12-08 11:25:19.697616: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      " 2/37 [>.............................] - ETA: 10s - loss: 0.7015 - accuracy: 0.35562021-12-08 11:25:19.982113: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2021-12-08 11:25:19.983281: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-12-08 11:25:19.984335: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/hparam_tuning/train/plugins/profile/2021_12_08_11_25_19\n",
      "\n",
      "2021-12-08 11:25:19.985503: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to logs/hparam_tuning/train/plugins/profile/2021_12_08_11_25_19/saumyas-MacBook-Pro.local.trace.json.gz\n",
      "2021-12-08 11:25:19.987297: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/hparam_tuning/train/plugins/profile/2021_12_08_11_25_19\n",
      "\n",
      "2021-12-08 11:25:19.987429: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to logs/hparam_tuning/train/plugins/profile/2021_12_08_11_25_19/saumyas-MacBook-Pro.local.memory_profile.json.gz\n",
      "2021-12-08 11:25:19.987947: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/hparam_tuning/train/plugins/profile/2021_12_08_11_25_19\n",
      "Dumped tool data for xplane.pb to logs/hparam_tuning/train/plugins/profile/2021_12_08_11_25_19/saumyas-MacBook-Pro.local.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/hparam_tuning/train/plugins/profile/2021_12_08_11_25_19/saumyas-MacBook-Pro.local.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/hparam_tuning/train/plugins/profile/2021_12_08_11_25_19/saumyas-MacBook-Pro.local.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/hparam_tuning/train/plugins/profile/2021_12_08_11_25_19/saumyas-MacBook-Pro.local.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/hparam_tuning/train/plugins/profile/2021_12_08_11_25_19/saumyas-MacBook-Pro.local.kernel_stats.pb\n",
      "\n",
      "37/37 [==============================] - 12s 287ms/step - loss: 0.6936 - accuracy: 0.4099 - val_loss: 0.6879 - val_accuracy: 0.6561\n",
      "Epoch 2/25\n",
      "37/37 [==============================] - 10s 273ms/step - loss: 0.6933 - accuracy: 0.5379 - val_loss: 0.6938 - val_accuracy: 0.3403\n",
      "Epoch 3/25\n",
      "37/37 [==============================] - 10s 278ms/step - loss: 0.6927 - accuracy: 0.4891 - val_loss: 0.6893 - val_accuracy: 0.6569\n",
      "Epoch 4/25\n",
      "37/37 [==============================] - 10s 276ms/step - loss: 0.6901 - accuracy: 0.5990 - val_loss: 0.6986 - val_accuracy: 0.3484\n",
      "Epoch 5/25\n",
      "37/37 [==============================] - 10s 275ms/step - loss: 0.6672 - accuracy: 0.6351 - val_loss: 0.6961 - val_accuracy: 0.3582\n",
      "Epoch 6/25\n",
      "37/37 [==============================] - 10s 279ms/step - loss: 0.6576 - accuracy: 0.5666 - val_loss: 0.7673 - val_accuracy: 0.6933\n",
      "Epoch 7/25\n",
      "37/37 [==============================] - 10s 278ms/step - loss: 0.5363 - accuracy: 0.8057 - val_loss: 0.7031 - val_accuracy: 0.7096\n",
      "Epoch 8/25\n",
      "37/37 [==============================] - 10s 282ms/step - loss: 0.4858 - accuracy: 0.8358 - val_loss: 0.6991 - val_accuracy: 0.7150\n",
      "Epoch 9/25\n",
      "37/37 [==============================] - 10s 276ms/step - loss: 0.4416 - accuracy: 0.8633 - val_loss: 0.7013 - val_accuracy: 0.7251\n",
      "Epoch 10/25\n",
      "37/37 [==============================] - 10s 280ms/step - loss: 0.4780 - accuracy: 0.8225 - val_loss: 0.6812 - val_accuracy: 0.6681\n",
      "Epoch 11/25\n",
      "37/37 [==============================] - 10s 273ms/step - loss: 0.4145 - accuracy: 0.8595 - val_loss: 0.6482 - val_accuracy: 0.7371\n",
      "Epoch 12/25\n",
      "37/37 [==============================] - 10s 275ms/step - loss: 0.4164 - accuracy: 0.8457 - val_loss: 0.7170 - val_accuracy: 0.6567\n",
      "Epoch 13/25\n",
      "37/37 [==============================] - 10s 283ms/step - loss: 0.3731 - accuracy: 0.8753 - val_loss: 0.7372 - val_accuracy: 0.7058\n",
      "Epoch 14/25\n",
      "37/37 [==============================] - 10s 269ms/step - loss: 0.3306 - accuracy: 0.9022 - val_loss: 0.7540 - val_accuracy: 0.7116\n",
      "Epoch 15/25\n",
      "37/37 [==============================] - 10s 274ms/step - loss: 0.3027 - accuracy: 0.9152 - val_loss: 0.8036 - val_accuracy: 0.7085\n",
      "Epoch 16/25\n",
      "37/37 [==============================] - 10s 275ms/step - loss: 0.2847 - accuracy: 0.9238 - val_loss: 0.7619 - val_accuracy: 0.7196\n",
      "Epoch 17/25\n",
      "37/37 [==============================] - 10s 277ms/step - loss: 0.2719 - accuracy: 0.9314 - val_loss: 0.8301 - val_accuracy: 0.7192\n",
      "Epoch 18/25\n",
      "37/37 [==============================] - 11s 289ms/step - loss: 0.2594 - accuracy: 0.9361 - val_loss: 0.8296 - val_accuracy: 0.7169\n",
      "Epoch 19/25\n",
      "37/37 [==============================] - 10s 274ms/step - loss: 0.2512 - accuracy: 0.9385 - val_loss: 0.8298 - val_accuracy: 0.7144\n",
      "Epoch 20/25\n",
      "37/37 [==============================] - 10s 278ms/step - loss: 0.2427 - accuracy: 0.9416 - val_loss: 0.8514 - val_accuracy: 0.7171\n",
      "Epoch 21/25\n",
      "37/37 [==============================] - 10s 272ms/step - loss: 0.2318 - accuracy: 0.9451 - val_loss: 0.8439 - val_accuracy: 0.7167\n",
      "Epoch 22/25\n",
      "37/37 [==============================] - 10s 267ms/step - loss: 0.2242 - accuracy: 0.9476 - val_loss: 0.8520 - val_accuracy: 0.7185\n",
      "Epoch 23/25\n",
      "37/37 [==============================] - 10s 266ms/step - loss: 0.2220 - accuracy: 0.9489 - val_loss: 0.8881 - val_accuracy: 0.7104\n",
      "Epoch 24/25\n",
      "37/37 [==============================] - 10s 280ms/step - loss: 0.2171 - accuracy: 0.9498 - val_loss: 0.9364 - val_accuracy: 0.7068\n",
      "Epoch 25/25\n",
      "37/37 [==============================] - 10s 273ms/step - loss: 0.2254 - accuracy: 0.9447 - val_loss: 0.9005 - val_accuracy: 0.7137\n",
      "27/27 [==============================] - 0s 17ms/step - loss: 0.8550 - accuracy: 0.7332\n",
      "--- Starting trial: run-1\n",
      "{'num_units': 100, 'dropout': 0.2, 'optimizer': 'adam'}\n",
      "2021-12-08 11:29:35.351159: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-12-08 11:29:35.351173: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-12-08 11:29:35.351595: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "Epoch 1/25\n",
      " 1/37 [..............................] - ETA: 49s - loss: 0.6731 - accuracy: 0.29202021-12-08 11:29:36.781508: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-12-08 11:29:36.781519: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      " 2/37 [>.............................] - ETA: 9s - loss: 0.6706 - accuracy: 0.4739 2021-12-08 11:29:37.060374: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2021-12-08 11:29:37.062246: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-12-08 11:29:37.065512: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/hparam_tuning/train/plugins/profile/2021_12_08_11_29_37\n",
      "\n",
      "2021-12-08 11:29:37.066621: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to logs/hparam_tuning/train/plugins/profile/2021_12_08_11_29_37/saumyas-MacBook-Pro.local.trace.json.gz\n",
      "2021-12-08 11:29:37.068879: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/hparam_tuning/train/plugins/profile/2021_12_08_11_29_37\n",
      "\n",
      "2021-12-08 11:29:37.068974: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to logs/hparam_tuning/train/plugins/profile/2021_12_08_11_29_37/saumyas-MacBook-Pro.local.memory_profile.json.gz\n",
      "2021-12-08 11:29:37.070082: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/hparam_tuning/train/plugins/profile/2021_12_08_11_29_37\n",
      "Dumped tool data for xplane.pb to logs/hparam_tuning/train/plugins/profile/2021_12_08_11_29_37/saumyas-MacBook-Pro.local.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/hparam_tuning/train/plugins/profile/2021_12_08_11_29_37/saumyas-MacBook-Pro.local.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/hparam_tuning/train/plugins/profile/2021_12_08_11_29_37/saumyas-MacBook-Pro.local.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/hparam_tuning/train/plugins/profile/2021_12_08_11_29_37/saumyas-MacBook-Pro.local.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/hparam_tuning/train/plugins/profile/2021_12_08_11_29_37/saumyas-MacBook-Pro.local.kernel_stats.pb\n",
      "\n",
      "37/37 [==============================] - 12s 287ms/step - loss: 0.6933 - accuracy: 0.5358 - val_loss: 0.7030 - val_accuracy: 0.3356\n",
      "Epoch 2/25\n",
      "37/37 [==============================] - 10s 270ms/step - loss: 0.6934 - accuracy: 0.4519 - val_loss: 0.6944 - val_accuracy: 0.3369\n",
      "Epoch 3/25\n",
      "37/37 [==============================] - 10s 276ms/step - loss: 0.6927 - accuracy: 0.4780 - val_loss: 0.6878 - val_accuracy: 0.6610\n",
      "Epoch 4/25\n",
      "37/37 [==============================] - 11s 301ms/step - loss: 0.6887 - accuracy: 0.6217 - val_loss: 0.6895 - val_accuracy: 0.6663\n",
      "Epoch 5/25\n",
      "37/37 [==============================] - 10s 279ms/step - loss: 0.6851 - accuracy: 0.5920 - val_loss: 0.7024 - val_accuracy: 0.3553\n",
      "Epoch 6/25\n",
      "37/37 [==============================] - 11s 285ms/step - loss: 0.6742 - accuracy: 0.4853 - val_loss: 0.8180 - val_accuracy: 0.7241\n",
      "Epoch 7/25\n",
      "37/37 [==============================] - 10s 279ms/step - loss: 0.5040 - accuracy: 0.8344 - val_loss: 0.6478 - val_accuracy: 0.7136\n",
      "Epoch 8/25\n",
      "37/37 [==============================] - 10s 270ms/step - loss: 0.4133 - accuracy: 0.8747 - val_loss: 0.6849 - val_accuracy: 0.7402\n",
      "Epoch 9/25\n",
      "37/37 [==============================] - 10s 268ms/step - loss: 0.3672 - accuracy: 0.8967 - val_loss: 0.6754 - val_accuracy: 0.7438\n",
      "Epoch 10/25\n",
      "37/37 [==============================] - 10s 268ms/step - loss: 0.3298 - accuracy: 0.9125 - val_loss: 0.6869 - val_accuracy: 0.7470\n",
      "Epoch 11/25\n",
      "37/37 [==============================] - 10s 266ms/step - loss: 0.3064 - accuracy: 0.9218 - val_loss: 0.7157 - val_accuracy: 0.7460\n",
      "Epoch 12/25\n",
      "37/37 [==============================] - 10s 268ms/step - loss: 0.2817 - accuracy: 0.9301 - val_loss: 0.7491 - val_accuracy: 0.7447\n",
      "Epoch 13/25\n",
      "37/37 [==============================] - 10s 269ms/step - loss: 0.2783 - accuracy: 0.9319 - val_loss: 0.6744 - val_accuracy: 0.7542\n",
      "Epoch 14/25\n",
      "37/37 [==============================] - 10s 282ms/step - loss: 0.2646 - accuracy: 0.9366 - val_loss: 0.7194 - val_accuracy: 0.7525\n",
      "Epoch 15/25\n",
      "37/37 [==============================] - 10s 280ms/step - loss: 0.2485 - accuracy: 0.9399 - val_loss: 0.7477 - val_accuracy: 0.7420\n",
      "Epoch 16/25\n",
      "37/37 [==============================] - 10s 271ms/step - loss: 0.2442 - accuracy: 0.9436 - val_loss: 0.8104 - val_accuracy: 0.7426\n",
      "Epoch 17/25\n",
      "37/37 [==============================] - 10s 272ms/step - loss: 0.2433 - accuracy: 0.9423 - val_loss: 0.6928 - val_accuracy: 0.7519\n",
      "Epoch 18/25\n",
      "37/37 [==============================] - 10s 275ms/step - loss: 0.2516 - accuracy: 0.9399 - val_loss: 0.8868 - val_accuracy: 0.6968\n",
      "Epoch 19/25\n",
      "37/37 [==============================] - 10s 271ms/step - loss: 0.2442 - accuracy: 0.9382 - val_loss: 0.8204 - val_accuracy: 0.7408\n",
      "Epoch 20/25\n",
      "37/37 [==============================] - 10s 271ms/step - loss: 0.2297 - accuracy: 0.9470 - val_loss: 0.7998 - val_accuracy: 0.7445\n",
      "Epoch 21/25\n",
      "37/37 [==============================] - 10s 271ms/step - loss: 0.3777 - accuracy: 0.8558 - val_loss: 0.7174 - val_accuracy: 0.6259\n",
      "Epoch 22/25\n",
      "37/37 [==============================] - 10s 273ms/step - loss: 0.3547 - accuracy: 0.8735 - val_loss: 0.7649 - val_accuracy: 0.7018\n",
      "Epoch 23/25\n",
      "37/37 [==============================] - 10s 272ms/step - loss: 0.2854 - accuracy: 0.9147 - val_loss: 0.8094 - val_accuracy: 0.7055\n",
      "Epoch 24/25\n",
      "37/37 [==============================] - 10s 276ms/step - loss: 0.2468 - accuracy: 0.9303 - val_loss: 0.8310 - val_accuracy: 0.7109\n",
      "Epoch 25/25\n",
      "37/37 [==============================] - 10s 271ms/step - loss: 0.2311 - accuracy: 0.9409 - val_loss: 0.8305 - val_accuracy: 0.7183\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 0.7177 - accuracy: 0.7605\n",
      "--- Starting trial: run-2\n",
      "{'num_units': 100, 'dropout': 0.5, 'optimizer': 'adam'}\n",
      "2021-12-08 11:33:51.042552: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-12-08 11:33:51.042564: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-12-08 11:33:51.042629: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "InternalError",
     "evalue": " Output 6 of type float does not match declared output type variant for node node sequential_3/bidirectional_3/forward_lstm_3/PartitionedCall (defined at var/folders/pl/26nmhlbj75j4gr5q76d2vg3h0000gn/T/ipykernel_8690/1114116650.py:14)  [Op:__inference_train_function_37524]\n\nFunction call stack:\ntrain_function\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pl/26nmhlbj75j4gr5q76d2vg3h0000gn/T/ipykernel_8690/461131688.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--- Starting trial: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logs/hparam_tuning/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m       \u001b[0msession_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/pl/26nmhlbj75j4gr5q76d2vg3h0000gn/T/ipykernel_8690/1542707868.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_dir, hparams)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# record the values used in this trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m280\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMETRIC_ACCURACY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/pl/26nmhlbj75j4gr5q76d2vg3h0000gn/T/ipykernel_8690/1114116650.py\u001b[0m in \u001b[0;36mtrain_test_model\u001b[0;34m(vocab_size, embedding_size, max_length, hparams)\u001b[0m\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     model.fit(train_padded, y_train, batch_size=256,epochs=25,class_weight=class_weights,validation_data=(valid_padded, y_valid),\n\u001b[0m\u001b[1;32m     15\u001b[0m     callbacks=[\n\u001b[1;32m     16\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logs/hparam_tuning'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# log metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tfnlp/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tfnlp/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tfnlp/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tfnlp/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tfnlp/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniforge3/envs/tfnlp/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tfnlp/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m:  Output 6 of type float does not match declared output type variant for node node sequential_3/bidirectional_3/forward_lstm_3/PartitionedCall (defined at var/folders/pl/26nmhlbj75j4gr5q76d2vg3h0000gn/T/ipykernel_8690/1114116650.py:14)  [Op:__inference_train_function_37524]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "\n",
    "for num_units in HP_NUM_UNITS.domain.values:\n",
    "  for dropout_rate in HP_DROPOUT.domain.values:\n",
    "    for optimizer in HP_OPTIMIZER.domain.values:\n",
    "      hparams = {\n",
    "          HP_NUM_UNITS: num_units,\n",
    "          HP_DROPOUT: dropout_rate,\n",
    "          HP_OPTIMIZER: optimizer,\n",
    "      }\n",
    "      run_name = \"run-%d\" % session_num\n",
    "      print('--- Starting trial: %s' % run_name)\n",
    "      print({h.name: hparams[h] for h in hparams})\n",
    "      run('logs/hparam_tuning/' + run_name, hparams)\n",
    "      session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
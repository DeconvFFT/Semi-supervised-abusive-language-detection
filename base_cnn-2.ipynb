{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8ac4b1a",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 1,
     "id": "e8ac4b1a",
     "kernelId": ""
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install scikit-learn\n",
    "# !pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23efbf50-c36e-4f29-9527-eb76bc6fec25",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "23efbf50-c36e-4f29-9527-eb76bc6fec25",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Embedding,Dense,Dropout,Bidirectional,GlobalMaxPool1D,GlobalAveragePooling1D, SpatialDropout1D,Input,Conv1D,MaxPooling1D,Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from sklearn.utils import class_weight\n",
    "import tensorflow as tf\n",
    "\n",
    "import emoji\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d84e7e8c-0390-4326-b83c-0feea1fc537f",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "d84e7e8c-0390-4326-b83c-0feea1fc537f",
     "kernelId": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/OLIDv1/olid-training-v1.0.tsv\", sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43727f4e-645a-4622-b79a-ff14719e3547",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "43727f4e-645a-4622-b79a-ff14719e3547",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reference: https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string\n",
    "CLEANR = re.compile('<.*?>') \n",
    "def cleanhtml(raw_html):\n",
    "    cleantext = re.sub(CLEANR, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "def preprocess(sent):\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz 0123456789',.\"\n",
    "    sent = sent.lower() \n",
    "    sent = cleanhtml(sent) # remove html tags \n",
    "    cleaned_sent_list = [char if char in alphabet else ' ' for char in sent] # remove all tags not in the alphabet\n",
    "    cleaned_sent = ''.join(cleaned_sent_list)\n",
    "    cleaned_sent = cleaned_sent.replace(\"n't\",' not') # replace words like \"isn't\" with \"is not\"\n",
    "    cleaned_sent = ' . '.join([x for x in cleaned_sent.split('.') if len(x)>0]) # remove multiple periods, and add spaces before and after a period\n",
    "    cleaned_sent = ' , '.join([x for x in cleaned_sent.split(',') if len(x)>0]) # add spaces before and after a comma\n",
    "    cleaned_sent = ' '.join(cleaned_sent.split()) # remove multiple spaces\n",
    "    return cleaned_sent\n",
    "\n",
    "def print_metrics(y,y_p):\n",
    "    print('Accuracy:',accuracy_score(y,y_p))\n",
    "    print('Precision:',precision_score(y,y_p))\n",
    "    print('Recall:',recall_score(y,y_p))\n",
    "    print('F1 score:',f1_score(y,y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb9f376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this block only for additional preprocessing steps\n",
    "\n",
    "def emoji_to_text(s):\n",
    "    s = emoji.demojize(s)\n",
    "    s = s.replace(':',' ')\n",
    "    s = s.replace('_',' ')    \n",
    "    s = ' '.join(s.split())\n",
    "    return s\n",
    "\n",
    "# loading twitter slang data\n",
    "slang_df = pd.read_csv('data/twitterSlang.csv')\n",
    "slang_dict = dict(zip(slang_df.slang, slang_df.formal_translation))\n",
    "\n",
    "def fix_slang(s):\n",
    "    s_list = s.split()\n",
    "    new_s_list = []\n",
    "    for word in s_list:\n",
    "        if word in slang_dict.keys():\n",
    "            new_s_list.append(slang_dict[word])\n",
    "        else:\n",
    "            new_s_list.append(word)\n",
    "            \n",
    "    return ' '.join(new_s_list)\n",
    "\n",
    "def preprocess(sent):\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz 0123456789',.\"\n",
    "    \n",
    "    sent = emoji_to_text(sent)\n",
    "    sent = fix_slang(sent)\n",
    "    sent = sent.lower() \n",
    "    sent = cleanhtml(sent) # remove html tags \n",
    "    cleaned_sent_list = [char if char in alphabet else ' ' for char in sent] # remove all tags not in the alphabet\n",
    "    cleaned_sent = ''.join(cleaned_sent_list)\n",
    "    cleaned_sent = cleaned_sent.replace(\"n't\",' not') # replace words like \"isn't\" with \"is not\"\n",
    "    cleaned_sent = ' . '.join([x for x in cleaned_sent.split('.') if len(x)>0]) # remove multiple periods, and add spaces before and after a period\n",
    "    cleaned_sent = ' , '.join([x for x in cleaned_sent.split(',') if len(x)>0]) # add spaces before and after a comma\n",
    "    cleaned_sent = ' '.join(cleaned_sent.split()) # remove multiple spaces\n",
    "    return cleaned_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9373eeb",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "f9373eeb",
     "kernelId": ""
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b77e134-3e72-4c5a-a0f6-3132285c8224",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "1b77e134-3e72-4c5a-a0f6-3132285c8224",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "x = df.tweet\n",
    "y = df.subtask_a.apply(lambda x: 1 if x=='OFF' else 0)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f8846df",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "2f8846df",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "data_train = [preprocess(tweet) for tweet in x_train]\n",
    "data_val = [preprocess(tweet) for tweet in x_val]\n",
    "\n",
    "n_features = 500\n",
    "vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,3),max_features=n_features)\n",
    "X_train = vectorizer.fit_transform(data_train)\n",
    "X_val = vectorizer.transform(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "548b7573",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "548b7573",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train.toarray().reshape([-1,n_features,1])\n",
    "X_val = X_val.toarray().reshape([-1,n_features,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b438b45",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "6b438b45",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "def model_cnn(n_features=500):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=5, strides=5, activation='relu', input_shape=(n_features,1)))\n",
    "    model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=5, strides=5, activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=5, strides=5, activation='relu'))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08b4d7c0",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "08b4d7c0",
     "kernelId": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.6410 - accuracy: 0.6693 - val_loss: 0.6358 - val_accuracy: 0.6644\n",
      "Epoch 2/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.6240 - accuracy: 0.6691 - val_loss: 0.6111 - val_accuracy: 0.6667\n",
      "Epoch 3/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.5898 - accuracy: 0.7063 - val_loss: 0.5863 - val_accuracy: 0.7085\n",
      "Epoch 4/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.5671 - accuracy: 0.7217 - val_loss: 0.5697 - val_accuracy: 0.7178\n",
      "Epoch 5/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.5505 - accuracy: 0.7303 - val_loss: 0.5676 - val_accuracy: 0.7228\n",
      "Epoch 6/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.5395 - accuracy: 0.7351 - val_loss: 0.5577 - val_accuracy: 0.7309\n",
      "Epoch 7/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.5318 - accuracy: 0.7413 - val_loss: 0.5596 - val_accuracy: 0.7226\n",
      "Epoch 8/25\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.5272 - accuracy: 0.7429 - val_loss: 0.5651 - val_accuracy: 0.7190\n",
      "Epoch 9/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.5198 - accuracy: 0.7505 - val_loss: 0.5755 - val_accuracy: 0.7256\n",
      "Epoch 10/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.5165 - accuracy: 0.7502 - val_loss: 0.5721 - val_accuracy: 0.7082\n",
      "Epoch 11/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.5065 - accuracy: 0.7557 - val_loss: 0.5737 - val_accuracy: 0.7105\n",
      "Epoch 12/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4934 - accuracy: 0.7687 - val_loss: 0.5756 - val_accuracy: 0.7105\n",
      "Epoch 13/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4890 - accuracy: 0.7697 - val_loss: 0.5876 - val_accuracy: 0.6956\n",
      "Epoch 14/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4715 - accuracy: 0.7804 - val_loss: 0.6016 - val_accuracy: 0.6964\n",
      "Epoch 15/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4586 - accuracy: 0.7878 - val_loss: 0.6119 - val_accuracy: 0.7198\n",
      "Epoch 16/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4472 - accuracy: 0.7980 - val_loss: 0.6147 - val_accuracy: 0.6850\n",
      "Epoch 17/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4295 - accuracy: 0.8047 - val_loss: 0.6326 - val_accuracy: 0.6825\n",
      "Epoch 18/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4038 - accuracy: 0.8221 - val_loss: 0.6655 - val_accuracy: 0.6966\n",
      "Epoch 19/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3834 - accuracy: 0.8328 - val_loss: 0.6983 - val_accuracy: 0.6878\n",
      "Epoch 20/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3657 - accuracy: 0.8446 - val_loss: 0.7644 - val_accuracy: 0.6281\n",
      "Epoch 21/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3559 - accuracy: 0.8444 - val_loss: 0.7619 - val_accuracy: 0.6918\n",
      "Epoch 22/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3208 - accuracy: 0.8642 - val_loss: 0.7664 - val_accuracy: 0.6644\n",
      "Epoch 23/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3058 - accuracy: 0.8735 - val_loss: 0.8104 - val_accuracy: 0.6571\n",
      "Epoch 24/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2847 - accuracy: 0.8848 - val_loss: 0.8664 - val_accuracy: 0.6871\n",
      "Epoch 25/25\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2631 - accuracy: 0.8943 - val_loss: 0.9060 - val_accuracy: 0.6644\n"
     ]
    }
   ],
   "source": [
    "model = model_cnn(500)\n",
    "history = model.fit(X_train.reshape([-1,n_features,1]),y_train,\n",
    "                    validation_data=(X_val.reshape([-1,n_features,1]),y_val),batch_size=256,epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1fe99a5c",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "1fe99a5c",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.77      0.75      2639\n",
      "           1       0.50      0.45      0.48      1333\n",
      "\n",
      "    accuracy                           0.66      3972\n",
      "   macro avg       0.62      0.61      0.61      3972\n",
      "weighted avg       0.66      0.66      0.66      3972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_val_p = model.predict(X_val)\n",
    "y_val_p = np.round(y_val_p).flatten()\n",
    "print(classification_report(y_val, y_val_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c53c152",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "6c53c152",
     "kernelId": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6644008056394763\n",
      "Precision: 0.5\n",
      "Recall: 0.45461365341335336\n",
      "F1 score: 0.4762278978388998\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_val, y_val_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83a2448b",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "1fe99a5c",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.80      0.77      2639\n",
      "           1       0.52      0.42      0.46      1333\n",
      "\n",
      "    accuracy                           0.67      3972\n",
      "   macro avg       0.63      0.61      0.61      3972\n",
      "weighted avg       0.66      0.67      0.66      3972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_val_p = model.predict(X_val)\n",
    "y_val_p = np.round(y_val_p).flatten()\n",
    "print(classification_report(y_val, y_val_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2eb03414",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "6c53c152",
     "kernelId": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6739677744209466\n",
      "Precision: 0.5175276752767528\n",
      "Recall: 0.42085521380345087\n",
      "F1 score: 0.4642118328506413\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_val, y_val_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f0e6b",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38aea61a-8223-4f7b-b89e-5febfe238beb",
   "metadata": {
    "gradient": {
     "id": "38aea61a-8223-4f7b-b89e-5febfe238beb",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"data/OLIDv1/testset-levela.tsv\", sep='\\t').set_index('id')\n",
    "y_test = pd.read_csv(\"data/OLIDv1/labels-levela.csv\", names = ['id','subtask_a']).set_index('id')\n",
    "df_test = df_test.join(y_test).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "800ebe6f-6d60-4052-b472-68f68e7e5346",
   "metadata": {
    "gradient": {
     "id": "800ebe6f-6d60-4052-b472-68f68e7e5346",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "x_test = df_test.tweet\n",
    "y_test = df_test.subtask_a.apply(lambda x: 1 if x=='OFF' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "862752e1-10cc-4bf1-a850-357af61bdc0d",
   "metadata": {
    "gradient": {
     "id": "862752e1-10cc-4bf1-a850-357af61bdc0d",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "data_test = [preprocess(tweet) for tweet in x_test]\n",
    "X_test = vectorizer.transform(data_test)\n",
    "X_test = X_test.toarray().reshape([-1,n_features,1])\n",
    "y_test_p = model.predict(X_test)\n",
    "y_test_p = np.round(y_test_p).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "881240b1",
   "metadata": {
    "gradient": {
     "id": "881240b1",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.82      0.81       620\n",
      "           1       0.50      0.46      0.48       240\n",
      "\n",
      "    accuracy                           0.72       860\n",
      "   macro avg       0.65      0.64      0.64       860\n",
      "weighted avg       0.71      0.72      0.72       860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_test_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42bd0a07",
   "metadata": {
    "gradient": {
     "id": "881240b1",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.83       620\n",
      "           1       0.56      0.47      0.51       240\n",
      "\n",
      "    accuracy                           0.75       860\n",
      "   macro avg       0.69      0.67      0.67       860\n",
      "weighted avg       0.74      0.75      0.74       860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_test_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d452ab",
   "metadata": {
    "gradient": {
     "id": "67d452ab",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
